\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
% Some optional stuff you might like/need.
\usepackage{microtype} % Improved Tracking and Kerning
% \usepackage[all]{hypcap}  % Fixes bug in hyperref caption linking
\usepackage{ccicons}  % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of your draft document, you
% have to enable the "chi_draft" option for the document class. To do this, change the very first
% line to: "\documentclass[chi_draft]{sigchi}". You can then place todo notes by using the "\todo{...}"
% command. Make sure to disable the draft option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Regression by Eye: Estimating Trends in Bivariate Visualizations}
\def\plainauthor{Removed for Review}
\def\emptyauthor{}
\def\plainkeywords{Information Visualization, Graphical Perception, Regression}
\def\plaingeneralterms{Visualization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{2}
\author{%
%  \alignauthor{Michael Correll\\
%    \affaddr{University of Washington}\\
%    \email{mcorrell@cs.washington.edu}}\\
%  \alignauthor{Jeffrey Heer\\
%    \affaddr{University of Washington}\\
%    \email{jheer@cs.washington.edu}}\\
}

\maketitle

\begin{abstract}
Observing trends and predicting future values are common tasks for viewers of bivariate data visualizations. As many charts do not explicitly include trend lines or related statistical summaries, viewers often visually estimate trends directly from a plot. How reliable are the inferences viewers draw when performing such \emph{regression by eye}? Do particular visualization designs or data features bias trend perception? We present a series of crowdsourced experiments that assess the accuracy of trends estimated using regression by eye across a variety of bivariate visualizations, and examine potential sources of bias in these estimations. We find that viewers accurately estimate trends in many standard visualizations of bivariate data, but that both visual features (e.g., ``within the bar'' bias) and data features (e.g., the presence of outliers) can result in visual estimates that systematically diverge from standard least-squares regression models. In most cases regression by eye performs similarly to ordinary least squares regression, but there are edge cases where perceptual and statistical regression systematically diverge. These results suggest that, while regression by eye can be a reliable way of estimating trends, in some common cases these estimates systematically differ from statistical methods of regression.
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous}

\keywords{\plainkeywords}

\section{Introduction}

%motivation here is poor. needs another turn of the crank.

One of the motivating examples for the power of information visualization is Anscombe's quartet\cite{anscombe1973graphs}, a set of four bivariate sets with nearly identical summary statistics, but with drastically different patterns when drawn as a scatterplot. Implicit in this example is that people have a robust ability to recognize visual patterns in information visualizations and, moreover, to employ this pattern recognition ability towards statistical ends. That is, an underlying assumption in information visualization is that people have the ability to \emph{visually} estimate \emph{statistical} quantities of interest, and can perform at least rough analogs of statistical tasks including model selection, identification of outliers, and estimation of summary statistics, entirely through visual inspection. Knowing the limits of this ``estimation by eye,'' is important for designers of information visualizations seeking to communicate statistical quantities, especially to the general audience. There are two important classes of limitation: either the visual estimations are \emph{inaccurate} for the use cases intended by the designer, or they are \emph{biased}, creating systematic over- or under-estimations.

The estimation of trends in bivariate data is an important analytical task, as it is the basis for many factors relevant to decision-making, such as prediction, imputation, and comparison. Unfortunately, model information is not always explicitly provided by designers of visualizations. When designers do include trend information (for instance, by annotating a scatterplot with a line of best fit), other statistics relevant to the model (such as $r$ values or confidence bands) may be absent. Viewers must therefore perform visual estimation to recover the relevant statistics not provided by the designers.

Even if designers specifically include all relevant information, the audience may lack the statistical expertise to interpret these values. As a further complication, visual presentation of bivariate data can influence the types of analysis made by viewers (for instance, viewers may be more likely to consider trends with line charts, and to compare individual values with bar charts \cite{zacks1999bars}). These design choices can also introduce bias (for instance, the visual asymmetry of bars can cause ``within-the-bar'' bias \cite{newman2012bar}). Designers need guidance both on how much accuracy they can expect from viewers making trend estimations by eye, as well as whether or not different types of bivariate visualizations can bias these estimations.

In this work, we describe a series of crowdsourced studies on the visual estimation of trends in common bivariate visualizations (scatterplots, area charts, and linegraphs). We report on the results of three studies, investigating estimation of trend slope, trend intercept, and the effect of outliers. We find that, while in most cases viewers make accurate estimations of trends, area charts introduce systematic underestimation of trend intercept, and outliers introduce ambiguity in how trends are estimated. These results suggest that there are several areas where human judgments diverge from the fitted models generated by techniques such as Ordinary Least Squares (OLS), and that the design of bivariate visualizations can introduce additional biases in these judgments.

\section{Related Work}

While there is a great deal of foundational work in visualization and graphical perception dealing with the estimation of individual values in visualizations (such as the height of the bar in a bar chart, or the angle of a line in a linegraph), there is comparatively little work on how viewers of visualizations perceive aggregate statistical quantities. Ariely~\cite{ariely2001seeing} suggests that, in concert with the perception of individual objects, we also collect information about the \emph{ensemble} properties of visual displays. Szafir et al.~\cite{szafir2016four} suggest that this \emph{ensemble coding} can afford the accurate estimation of \emph{summary statistics} in visualizations. However, visualizations with good performance for summary tasks may not result in good performance at point tasks, and vice versa~\cite{albers2014task,fuchs2013evaluation}. A further difficulty is that tasks requiring estimations of values in visualizations (where there is a single correct answer) are qualitatively different from tasks requiring predictions (where differing mental models and priors can results in a multitude of potential valid responses).

Scatterplots are a standard design for visualization bivariate data, with a multitude of design parameters that affect their suitability for aggregate tasks~\cite{cleveland1984many}. Prior work has confirmed that viewers can make use of scatterplots to perform prediction tasks (which tacitly rely on estimation of trend) in ways that are robust to both noise~\cite{harvey1997effects} and problem frame~\cite{lewandowsky2011popular}. However, the heuristics used when performing these prediction tasks (such as the anchor-and-adjust method~\cite{bolger1993context}) can introduce biases. Similarly, the design of scatterplots (for instance, the decision to encode class membership with color or with shape) can also impact performance at aggregate tasks~\cite{gleicher2013perception,lewandowsky1989discriminating}. The aspect ratio of graphs can also bias judgments about trend~\cite{beattie2002impact}--- wider aspect ratios can cause viewers to underestimate the severity of effects, when compared to narrower charts. Another bias in prediction tasks is the ``within-the-bar'' bias \cite{newman2012bar}: for visually asymmetric visualizations such as bar charts, points contained within the visual area of the bar glyph are perceived as likelier than those outside the glyph. 

Recent work in the visualization community has focused on the perception of correlation in scatterplots. Rensink et al.~\cite{rensink2010perception} show that viewers can estimate correlation with some accuracy in scatterplots. Harrison et al. \cite{harrison2014ranking} extend this finding to other visualization types, although a reanalysis by Kay \& Heer~\cite{kay2016beyond} indicates that, for many of the more esoteric bivariate visualizations, performance at this task is poor. Estimation of correlation can also be biased by the design of scales~\cite{cleveland1982variables}--- the whitespace and aspect ratio changes introduced by expanding the scale of the axes can cause overestimation of correlation.

Our task of trend estimation combines elements of both prediction and correlation estimation tasks. As with prediction, there is not necessarily an unambiguous correct estimation of trend (different modeling and regression methods can produce different trend lines). As with correlation, viewers must make holistic judgments about the dataset in a way that (as per Harrison et al.) likely relies on a set of key visual proxies. And, as with both tasks, we believe that trend estimation can be biased through conscious or unconscious design choices.  

\section{General Methods}

In order to assess the ability of viewers of visualizations to estimate trend, we conducted a series of three crowdsourced experiments on Amazon's Mechanical Turk platform. We designed these experiments to establish a baseline for performance at regression by eye, and to examine potential sources of bias for these estimates. Crowd-sourcing is a valuable tool for graphical perception experiments, as it affords flexibilities and scales that would be difficult for in-person experiments~\cite{heer2010crowdsourcing}. Crowd-sourced graphical perception experiments produce results that are largely in keeping with prior, lab-based work~\cite{talbot2014four}.

In each experiment, we presented participants with a set of bivariate visualizations based on an underlying set of points. We wished to have precise and orthogonal control over relevant statistics such as noise, slope, and sample size, while maintaining the appearance of a ``natural'' distribution of points. Existing methods for generating points for related experimental tasks such as estimation of correlations (e.g., Harrison et al.~\cite{harrison2014ranking}) do not afford this orthogonality: e.g., Pearson's $r$ is correlated with the slope (points with identical residuals but different slopes of their linear fits will have differing $r$ values). We also wished to have a fair comparison between visual estimations and the results of OLS. We therefore used the standard model of OLS to generate points, namely that $y=\beta x + \epsilon$, where $\epsilon$ is an error term that follows a Gaussian model.

Therefore, for each point set, we created a set of residual values, sampled evenly from a discretized Gaussian. This set was permuted, and then applied to the points along with a target linear trend. As heteroskedasticity introduced through permutation could alter the actual linear trend away from the target trend, we performed rejection sampling to ensure that the trend of the resulting points were within $10^{-7}$ of the target trend. We reused these residuals across all different trend types (linear, quadratic, or trigonometric). Except where noted, we selected trend lines that were centered in the image: that is, for a horizontal data extent of $[0,1]$, $f(0.5) = 0.5$. In all experiments, we desired control over the direction of the trend. For linear fits, this is the slope of the trend line. For quadratic fits, this was the curvature, as controlled by the coefficient of the second degree term. For trignomoetric fits, this was the amplitude of a cosine function (with negative amplitude corresponding to negative slopes, and positive to positive slopes). These terms create similar relationships, such that a value of $0$ is the line $f(x)=0.5$, a value of $1$ goes from $f(0)=0$ to $f(1)=1$, and $-1$ has the inverse relationship.

In all experiments, participants entered in information using a slider without tick marks, in order to avoid anchoring effects \cite{matejka2016effect}. Moving the slider adjusts a purple trend line by adjusting its slope (in experiments 1 and 3) or its y-intercept (in experiment 2). After moving the slider, participants had to confirm their choice of trend line. The central measure is accuracy, or the difference between the selected slope and the slope of the series if the residuals were removed. We chose this design over a standard binary forced choice design for its greater expressiveness, as well as for the interactive feedback of adjusting the fit by hand. We avoided more expressive designs (such as freeform drawing or other elicitation techniques) in order to constrain the model-space of participants, and to reduce the impact of factors such as accuracy in motor movements and heterogeneity in input devices.

\subsection{Participants}

We limited the Mechanical Turk participant pool to subjects from within the United States, with a prior task approval rating of at least 90\%. For validation, certain trials contained the actual (OLS) trend line. We excluded the results of participants with poor performance on these validation questions. Across the three reported experiments and pilots, we performed 7 such exclusions. We recruited additional participants to replace these poor performers. Based on timings from internal piloting, we paid each participant \$2 for their participation, for an intended rate of \$8/hour. 

We analyzed 48 participants for each experiment (excluding rejections), for a total of 144 (98 male, 43 female, 3 who declined to stat, $M_{age}= 33.2$,$SD_{age}=8.8$) participants. The 10 had graduate or professional degrees, 69 had college degrees, 40 had at least some college, and the remaining 25 had high school diplomas. After completing the main experimental task, participants were asked to self-assess their familiarity with graphs and charts, on a 5 point Likert scale. The plurality (64) rated themselves as ``3. Some familiarity,'' and none rated themselves with the maximum rating of ``5. A great deal of experience.'' 

\section{1. Estimation of Slope is Reliable \\ Across Visualizations}

We designed our first experiment to examine how accurate participants were are estimating the direction (slope, amplitude, or curvature) of trends in bivariate visualizations. We examined a variety of common bivariate visualizations. In addition to linear trends, we examined more complex relationships such as quadratic and trignometric trends.

We presented participants with a series of bivariate visualizations, and adjusted a slider to fit the perceived trend in the points. Visualizations were one of a scatterplot, linegraph, or area chart (with the filled area below the line). Visualizations consisted of one of three types of trend (linear, quadratic, or trigonometric). For each stimulus, participants adjusted a slider that controlled the slope of a rendered trend line (or, in the case of the quadratic trends, the curvature, or the trigonometric, the positive/negative amplitude). 

Participants saw one of each combination of 3 chart types (scatterplot, line chart, or area chart), 8 possible slopes $\beta = \pm \{0.1,0.2,0.4,0.8\}$, 4 bandwidths of Gaussian residuals $\sigma = \{0.05,0.1,0.15,0.2\}$, for a total of 96 stimuli. Type of trend (linear, quadratic, or trignometric) was a random factor, with 32 stimuli of each factor level.

\subsection{Hypotheses}

We had three hypotheses for the first experiment:
\begin{enumerate}
	\item \textbf{As the bandwidth of the residuals increased, accuracy would decrease}. Increasing the bandwidth of the residuals results in a lower correlation coefficient and higher perceived noise in the bivariate data. Prior work indicates that these related measures correspond to decreased accuracy for aggregate tasks in bivariate visualizations \cite{albers2014task,harrison2014ranking}. 
	\item \textbf{More complex relationships would result in lower accuracy}. 
\end{enumerate}
\subsection{Results}

\section{2. Estimation of Intercept is Subject to \\ ``Within The Bar'' Bias}

``Within the bar'' bias is a perceptual bias in bar charts where points contained in the visual area of the glyph of the bar are seen as likelier than points outside of the glyph. Newman \& Scholl \cite{newman2012bar} encountered this bias for a sampling task: ``how likely is this point to have been drawn from the distribution represented by this bar?'' Correll \& Gleicher \cite{correll2014error} likewise encountered this bias for inferential tasks: ``how likely is the population mean to take a particular value, given the sample represented by this bar?'' We believed that this bias would likewise occur in regression tasks in visually asymmetric visualizations such as area charts. The slope estimation task from the previous experiment would not capture this bias, as there is no method for participants to indicate a \emph{uniform} underestimation in trends; decreasing the slope would cause underestimation at the beginning of the plot but not the end, and vice versa. We therefore designed this experiment to measure estimates of the y-intercepts of trends. The within-the-bar bias here would then appear as systematic underestimation of intercept in area charts.

As with the previous experiment, we presented participants with a series of bivariate visualizations. However, instead of estimating the \emph{slope} of the points, participants estimated the \emph{y-intercept} of the trend line. For each trial, we added a uniform offset to the points in the bivariate visualization in the data range $[-0.25,0.25]$. The rendered trend line was initially placed with the correct slope, and such that $f(0.5)=0.5$ in data space. Participants adjusted a slider controlling the vertical offset of this trend line. 

The plots had the same factor levels as the previous experiment, for a total of 96 stimuli per participant. The uniform offset was an additional random factor for each stimulus. 

\subsection{Hypotheses}

We had one hypothesis for the second experiment:
\begin{enumerate}
	\item \textbf{Area charts would be subject to within the bar bias.} That is, participants would estimate lower values of y-intercepts of trends than with line charts and scatterplots. 
\end{enumerate}
\subsection{Results}

\section{3. Estimation of Slope is Sensitive to Outliers}

OLS regression operates under the assumption that there is a unimodal, symmetric distribution of residuals surrounding the line of best fit. Extreme outliers violate this assumption, and can result in trend lines that are substantially different from those produced by more robust methods. Visual inspection can accurately identify certain classes of outliers \cite{albers2014task}. Therefore, regression by eye may afford the estimation of both outlier-robust and outlier-sensitive trends. However, cognitive biases such as anchoring (the tendency to overweight the first set of information), availability (the tendency to overweight more recent or extreme information), and the hot-hand fallacy (the tendency to assume that trends of high or low values with continue) can impact how viewers categorize and utilize outliers in their estimations. These biases can impact forecasting tasks~\cite{campbell2009anchoring,ji2001culture}. That is, regression by eye may not uniformly weight outliers, depending on their position in the plot.

This experiment was largely identical to experiment 1 above, except we designated 0, 5, 10, or 15 points at the very beginning, first third, or end of the series as outliers, and placed them randomly in the top or bottom 10\% of the visual area of the visualization (whichever was farthest from the trend line). We then calculated the outlier-sensitive line of best fit, as well as the intersection between this new trend line and the original, outlier-less trend line. As with experiment 1, the participants controlled the slope of a rendered trend line with a slider. However, rather than being placed such that $f(0.5)=0.5$, we offset the trend line to the intersection of the robust (without outliers) and non-robust (including outliers) trend line. This allowed participants to express both types of fit with the same slider interaction, while affording estimations beyond a simple interpolation of both trends. This expressiveness is only afforded by linear fits of, and so we did not consider quadratic or trigonometric trends in this experiment.

Similar to the previous experiments, participants saw one of each of the three graph types, slope, and Gaussian residual bandwidth. The four outlier quantities $\{0,5,10,15\}$ was an additional factor. To maintain a manageable number of stimuli (piloting showed evidence of fatigue for more than 100 stimuli per trial), both the sign of the trend line (positive or negative), and the location of the outliers (beginning, first third, or end) were random factors, with each level apportioned to half and one third of the stimuli respectively. This resulted in 96 total stimuli, in line with prior experiments.

\subsection{Hypotheses}

We had three hypotheses for the third experiment:
\begin{enumerate}
	\item \textbf{Participant estimation would be closer to an outlier-less robust trend line, than the non-robust OLS trend line.} That is, we assumed that in general, participants would ignore or downweight outliers when performing regression by eye.
	\item \textbf{As the number of outliers increased, estimations would be closer to the non-robust OLS fit.} We speculated on the existence of a ``tipping point'' of outlier density, beyond which participants would avoid robust fits, and interpolate between the robust and OLS fits.
	\item \textbf{Outliers at the end of the chart would result in estimations closer to the OLS fit than outliers in other locations on the plot.} Prior work on cognitive biases in forecasting tasks suggests that viewers may give more credence to more recent outliers (as indicative as new anchors points, or of an emerging ``streak''). We therefore believed that these points would be more heavily weighted, and the resulting estimation closer to the outlier-sensitive OLS line, than outliers ``earlier'' in the plot.
\end{enumerate}
\subsection{Results}

\section{Discussion}
\subsection{Limitations \& Future Work}
\subsection{Conclusion}
\section{Acknowledgments}

Omitted for review.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance{}


% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
