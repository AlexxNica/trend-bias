\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
% Some optional stuff you might like/need.
\usepackage{microtype} % Improved Tracking and Kerning
% \usepackage[all]{hypcap}  % Fixes bug in hyperref caption linking
\usepackage{ccicons}  % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of your draft document, you
% have to enable the "chi_draft" option for the document class. To do this, change the very first
% line to: "\documentclass[chi_draft]{sigchi}". You can then place todo notes by using the "\todo{...}"
% command. Make sure to disable the draft option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Regression by Eye: Estimating Trends in Bivariate Visualizations}
\def\plainauthor{Removed for Review}
\def\emptyauthor{}
\def\plainkeywords{Information Visualization, Graphical Perception, Regression}
\def\plaingeneralterms{Visualization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{2}
\author{%
%  \alignauthor{Michael Correll\\
%    \affaddr{University of Washington}\\
%    \email{mcorrell@cs.washington.edu}}\\
%  \alignauthor{Jeffrey Heer\\
%    \affaddr{University of Washington}\\
%    \email{jheer@cs.washington.edu}}\\
}

\maketitle

\begin{abstract}
Observing trends and predicting future values are common tasks for viewers of bivariate data visualizations. As many charts do not explicitly include trend lines or related statistical summaries, viewers often visually estimate trends directly from a plot. How reliable are the inferences viewers draw when performing such \emph{regression by eye}? Do particular visualization designs or data features bias trend perception? We present a series of crowdsourced experiments that assess the accuracy of trends estimated using regression by eye across a variety of bivariate visualizations, and examine potential sources of bias in these estimations. We find that viewers accurately estimate trends in many standard visualizations of bivariate data, but that both visual features (e.g., ``within the bar'' bias) and data features (e.g., the presence of outliers) can result in visual estimates that systematically diverge from standard least-squares regression models. In most cases regression by eye performs similarly to ordinary least squares regression, but there are edge cases where perceptual and statistical regression systematically diverge. These results suggest that, while regression by eye can be a reliable way of estimating trends, in some common cases these estimates systematically differ from statistical methods of regression.
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous}

\keywords{\plainkeywords}

\input{./figures/figures.tex}

\section{Introduction}

%motivation here is poor. needs another turn of the crank.

One of the motivating examples for the power of information visualization is Anscombe's quartet\cite{anscombe1973graphs}, a set of four bivariate sets with nearly identical summary statistics, but with drastically different patterns when drawn as a scatterplot. Implicit in this example is that people have a robust ability to recognize visual patterns in information visualizations and, moreover, to employ this pattern recognition ability towards statistical ends. That is, an underlying assumption in information visualization is that people have the ability to \emph{visually} estimate \emph{statistical} quantities of interest, and can perform at least rough analogs of statistical tasks including model selection, identification of outliers, and estimation of summary statistics, entirely through visual inspection. Knowing the limits of this ``estimation by eye,'' is important for designers of information visualizations seeking to communicate statistical quantities, especially to the general audience. There are two important classes of limitation: either the visual estimations are \emph{inaccurate} for the use cases intended by the designer, or they are \emph{biased}, creating systematic over- or under-estimations.

The estimation of trends in bivariate data is an important analytical task, as it is the basis for many factors relevant to decision-making, such as prediction, imputation, and comparison. Unfortunately, model information is not always explicitly provided by designers of visualizations. When designers do include trend information (for instance, by annotating a scatterplot with a line of best fit), other statistics relevant to the model (such as $r$ values or confidence bands) may be absent. Viewers must therefore perform visual estimation to recover the relevant statistics not provided by the designers.

Even if designers specifically include all relevant information, the audience may lack the statistical expertise to interpret these values. As a further complication, visual presentation of bivariate data can influence the types of analysis made by viewers (for instance, viewers may be more likely to consider trends with line charts, and to compare individual values with bar charts \cite{zacks1999bars}). These design choices can also introduce bias (for instance, the visual asymmetry of bars can cause ``within-the-bar'' bias \cite{newman2012bar}). Designers need guidance both on how much accuracy they can expect from viewers making trend estimations by eye, as well as whether or not different types of bivariate visualizations can bias these estimations.

In this work, we describe a series of crowdsourced studies on the visual estimation of trends in common bivariate visualizations (scatterplots, area charts, and linegraphs). We report on the results of three studies, investigating estimation of trend slope, trend intercept, and the effect of outliers. We find that, while in most cases viewers make accurate estimations of trends, area charts introduce systematic underestimation of trend intercept, and outliers introduce ambiguity in how trends are estimated. These results suggest that there are several areas where human judgments diverge from the fitted models generated by techniques such as Ordinary Least Squares (OLS), and that the design of bivariate visualizations can introduce additional biases in these judgments.

\section{Related Work}

While there is a great deal of foundational work in visualization and graphical perception dealing with the estimation of individual values in visualizations (such as the height of the bar in a bar chart, or the angle of a line in a linegraph), there is comparatively little work on how viewers of visualizations perceive aggregate statistical quantities. Ariely~\cite{ariely2001seeing} suggests that, in concert with the perception of individual objects, we also collect information about the \emph{ensemble} properties of visual displays. Szafir et al.~\cite{szafir2016four} suggest that this \emph{ensemble coding} can afford the accurate estimation of \emph{summary statistics} in visualizations. However, visualizations with good performance for summary tasks may not result in good performance at point tasks, and vice versa~\cite{albers2014task,fuchs2013evaluation}. A further difficulty is that tasks requiring estimations of values in visualizations (where there is a single correct answer) are qualitatively different from tasks requiring predictions (where differing mental models and priors can results in a multitude of potential valid responses).

Scatterplots are a standard design for visualization bivariate data, with a multitude of design parameters that affect their suitability for aggregate tasks~\cite{cleveland1984many}. Prior work has confirmed that viewers can make use of scatterplots to perform prediction tasks (which tacitly rely on estimation of trend) in ways that are robust to both noise~\cite{harvey1997effects} and problem frame~\cite{lewandowsky2011popular}. However, the heuristics used when performing these prediction tasks (such as the anchor-and-adjust method~\cite{bolger1993context}) can introduce biases. Similarly, the design of scatterplots (for instance, the decision to encode class membership with color or with shape) can also impact performance at aggregate tasks~\cite{gleicher2013perception,lewandowsky1989discriminating}. The aspect ratio of graphs can also bias judgments about trend~\cite{beattie2002impact}--- wider aspect ratios can cause viewers to underestimate the severity of effects, when compared to narrower charts. Another bias in prediction tasks is the ``within-the-bar'' bias \cite{newman2012bar}: for visually asymmetric visualizations such as bar charts, points contained within the visual area of the bar glyph are perceived as likelier than those outside the glyph. 

Recent work in the visualization community has focused on the perception of correlation in scatterplots. Rensink et al.~\cite{rensink2010perception} show that viewers can estimate correlation with some accuracy in scatterplots. Harrison et al. \cite{harrison2014ranking} extend this finding to other visualization types, although a reanalysis by Kay \& Heer~\cite{kay2016beyond} indicates that, for many of the more esoteric bivariate visualizations, performance at this task is poor. Estimation of correlation can also be biased by the design of scales~\cite{cleveland1982variables}--- the whitespace and aspect ratio changes introduced by expanding the scale of the axes can cause overestimation of correlation.

Our task of trend estimation combines elements of both prediction and correlation estimation tasks. As with prediction, there is not necessarily an unambiguous correct estimation of trend (different modeling and regression methods can produce different trend lines). As with correlation, viewers must make holistic judgments about the dataset in a way that (as per Harrison et al.) likely relies on a set of key visual proxies. And, as with both tasks, we believe that trend estimation can be biased through conscious or unconscious design choices.  

\section{General Methods}

\expFig

In order to assess the ability of viewers of visualizations to estimate trend, we conducted a series of three crowdsourced experiments on Amazon's Mechanical Turk platform. We designed these experiments to establish a baseline for performance at regression by eye, and to examine potential sources of bias for these estimates. Crowd-sourcing is a valuable tool for graphical perception experiments, as it affords flexibilities and scales that would be difficult for in-person experiments~\cite{heer2010crowdsourcing}. Crowd-sourced graphical perception experiments produce results that are largely in keeping with prior, lab-based work~\cite{talbot2014four}.

In each experiment, we presented participants with a set of bivariate visualizations based on an underlying set of points. We wished to have precise and orthogonal control over relevant statistics such as noise, slope, and sample size, while maintaining the appearance of a ``natural'' distribution of points. Existing methods for generating points for related experimental tasks such as estimation of correlations (e.g., Harrison et al.~\cite{harrison2014ranking}) do not afford this orthogonality: e.g., Pearson's $r$ is correlated with the slope (points with identical residuals but different slopes of their linear fits will have differing $r$ values). We also wished to have a fair comparison between visual estimations and the results of OLS. We therefore used the standard model of OLS to generate points, namely that $y=\beta x + \epsilon$, where $\epsilon$ is an error term that follows a Gaussian model.

Therefore, for each point set, we created a set of residual values, sampled evenly from a discretized Gaussian. This set was permuted, and then applied to the points along with a target linear trend. As heteroskedasticity introduced through permutation could alter the actual linear trend away from the target trend, we performed rejection sampling to ensure that the trend of the resulting points were within $10^{-7}$ of the target trend. We reused these residuals across all different trend types (linear, quadratic, or trigonometric). Except where noted, we selected trend lines that were centered in the image: that is, for a horizontal data extent of $[0,1]$, $f(0.5) = 0.5$. In all experiments, we desired control over the direction of the trend. For linear fits, this is the slope of the trend line. For quadratic fits, this was the curvature, as controlled by the coefficient of the second degree term. For trignomoetric fits, this was the amplitude of a cosine function (with negative amplitude corresponding to negative slopes, and positive to positive slopes). These terms create similar relationships, such that a value of $0$ is the line $f(x)=0.5$, a value of $1$ goes from $f(0)=0$ to $f(1)=1$, and $-1$ has the inverse relationship.

In all experiments, participants entered in information using a slider without tick marks, in order to avoid anchoring effects \cite{matejka2016effect}. Moving the slider adjusts a purple trend line by adjusting its slope (in experiments 1 and 3) or its y-intercept (in experiment 2). Fig. \ref{fig:experiment} shows an example experimental task. After moving the slider, participants had to confirm their choice of trend line. The central measure is accuracy, or the difference between the selected slope and the slope of the series if the residuals were removed. We chose this design over a standard binary forced choice design for its greater expressiveness, as well as for the interactive feedback of adjusting the fit by hand. We avoided more expressive designs (such as freeform drawing or other elicitation techniques) in order to constrain the model-space of participants, and to reduce the impact of factors such as accuracy in motor movements and heterogeneity in input devices.

\subsection{Participants}

We limited the Mechanical Turk participant pool to subjects from within the United States, with a prior task approval rating of at least 90\%. For validation, certain trials contained the actual (OLS) trend line. We excluded the results of participants with poor performance on these validation questions. Across the three reported experiments and pilots, we performed 7 such exclusions. We recruited additional participants to replace these poor performers. Based on timings from internal piloting, we paid each participant \$2 for their participation, for an intended rate of \$8/hour. 

We analyzed 48 participants for each experiment (excluding rejections), for a total of 144 (98 male, 43 female, 3 who declined to stat, $M_{age}= 33.2$,$SD_{age}=8.8$) participants. The 10 had graduate or professional degrees, 69 had college degrees, 40 had at least some college, and the remaining 25 had high school diplomas. After completing the main experimental task, participants were asked to self-assess their familiarity with graphs and charts, on a 5 point Likert scale. The plurality (64) rated themselves as ``3. Some familiarity,'' and none rated themselves with the maximum rating of ``5. A great deal of experience.'' 

\section{1. Estimation of Slope is Reliable \\ Across Visualizations}

We designed our first experiment to examine how accurate participants were are estimating the direction (slope, amplitude, or curvature) of trends in bivariate visualizations. We examined a variety of common bivariate visualizations. In addition to linear trends, we examined more complex relationships such as quadratic and trigonometric trends.

We presented participants with a series of bivariate visualizations, and adjusted a slider to fit the perceived trend in the points. Visualizations were one of a scatterplot, linegraph, or area chart (with the filled area below the line). Visualizations consisted of one of three types of trend (linear, quadratic, or trigonometric). For each stimulus, participants adjusted a slider that controlled the slope of a rendered trend line (or, in the case of the quadratic trends, the curvature, or the trigonometric, the positive/negative amplitude). 

Participants saw one of each combination of 3 chart types (scatterplot, line chart, or area chart), 8 possible slopes $\beta = \pm \{0.1,0.2,0.4,0.8\}$, 4 bandwidths of Gaussian residuals $\sigma = \{0.05,0.1,0.15,0.2\}$, for a total of 96 stimuli, with an additional 4 validation stimuli otherwise excluded from analysis. Type of trend (linear, quadratic, or trigonometric) was a random factor, with 32 stimuli of each factor level.

\subsection{Hypotheses}

We had three hypotheses for the first experiment:
\begin{enumerate}
	\item \textbf{As the bandwidth of the residuals increased, accuracy would decrease}. Increasing the bandwidth of the residuals results in a lower correlation coefficient and higher perceived noise in the bivariate data. Prior work indicates that these related measures correspond to decreased accuracy for aggregate tasks in bivariate visualizations \cite{albers2014task,harrison2014ranking}. 
	\item \textbf{More complex relationships would result in lower accuracy}. Quadratic and trigonometric relationships are visually more complex than linear relationships, and often require more complex statistical methods to analyze. We anticipated that estimation of these less familiar relationships would therefore be more difficult than in the linear case.
	\item \textbf{Estimations would be unbiased}. That is, there would be no systematic over- or underestimation of trends. 
\end{enumerate}
\subsection{Results}
We performed a three-way analysis of covariance (ANCOVA) of the effect of residual bandwidth, graph type and trend type on unsigned error in estimation of trend lines. We included participant ID and the actual slope of the trend line as covariates.

Our results support our first hypothesis: \textbf{larger residuals reduce accuracy at regression by eye}. Bandwidth of the Gaussian used to generate residuals was a significant main effect ($F(1,4554)=950$), and the trimmed mean of absolute error increased monotonically with this bandwidth, from $7\%$ of the actual slope of the trend line when the bandwidth was $0.05$, to $32\%$ when the bandwidth was $0.20$. Figure \ref{fig:sigma} illustrates these results.

Our results fail to support our second hypothesis: \textbf{there was no statistically significant difference in estimation accuracy among linear, quadratic, or trigonometric trends}. Fit type was only a marginal main effect ($F(2,4554)=2.60$, $p=0.074$), and a post-hoc Tukey's test of Honest Significant Difference (HSD) did not identify any significant pairwise-interactions. The trimmed mean of absolute estimation error of linear fits was $17\%$ of the actual slope, in the middle of the $16\%$ error for quadratic fits, and $18\%$ for trignometric fits, providing evidence that the relative unfamiliarity of non-linear trends does not have a significant (in the ordinary sense) impact on performance.

Our results support our third hypothesis: \textbf{there was no statistically significant bias in estimations}. Participants saw an equal number of stimuli with positive and negative trend lines. If estimates of these trends were unbiased, we would expect the average signed error to be close to zero. The average signed error was $0.0008$, far less than the fidelity of the slider used to input guesses (which had $\Delta=0.01$). A Student's T test failed to support ($p=0.70$) the hypothesis that $mu_{error}\ne0$.

\section{2. Estimation of Intercept is Subject to \\ ``Within The Bar'' Bias}

``Within the bar'' bias is a perceptual bias in bar charts where points contained in the visual area of the glyph of the bar are seen as likelier than points outside of the glyph. Newman \& Scholl \cite{newman2012bar} encountered this bias for a sampling task: ``how likely is this point to have been drawn from the distribution represented by this bar?'' Correll \& Gleicher \cite{correll2014error} likewise encountered this bias for inferential tasks: ``how likely is the population mean to take a particular value, given the sample represented by this bar?'' We believed that this bias would likewise occur in regression tasks in visually asymmetric visualizations such as area charts. The slope estimation task from the previous experiment would not capture this bias, as there is no method for participants to indicate a \emph{uniform} underestimation in trends; decreasing the slope would cause underestimation at the beginning of the plot but not the end, and vice versa. We therefore designed this experiment to measure estimates of the y-intercepts of trends. The within-the-bar bias here would then appear as systematic underestimation of intercept in area charts.

As with the previous experiment, we presented participants with a series of bivariate visualizations. However, instead of estimating the \emph{slope} of the points, participants estimated the \emph{y-intercept} of the trend line. For each trial, we added a uniform offset to the points in the bivariate visualization in the data range $[-0.25,0.25]$. The rendered trend line was initially placed with the correct slope, and such that $f(0.5)=0.5$ in data space. Participants adjusted a slider controlling the vertical offset of this trend line. 

The plots had the same factor levels as the previous experiment, for a total of 96 stimuli per participant, with an additional 4 validation stimuli otherwise excluded from analysis. The uniform offset was an additional random factor for each stimulus. 

\subsection{Hypotheses}

We had one hypothesis for the second experiment:
\begin{enumerate}
	\item \textbf{Area charts would be subject to within the bar bias.} That is, participants would estimate lower values of y-intercepts of trends than with line charts and scatterplots. 
\end{enumerate}
\subsection{Results}
We performed a one-way ANCOVA of the effect of graph type on \emph{signed} error. We included residual bandwidth as a covariate, and participant ID as a random factor. 

Our results support our first hypothesis: \textbf{participants systematically overestimated the intercept of trends in area charts, but not in scatterplots or linegraphs}. Graph type was a significant main effect on signed error ($F(2,2431)=27$, $p<0.001$). A post-hoc Tukey's HSD confirmed significant differences in error between the area chart and the other two chart types, but not between scatterplots and linegraphs. The trimmed mean of the signed error of estimations made with area charts was an underestimation of $-0.02$, compared to a trimmed mean of $0.002$ for the other two conditions. This underestimation corresponded to unsigned errors more than twice as large in area charts (trimmed mean of $0.04$ for intercepts that ranged from $\{-0.25,0.25\}$, compared to $0.02$ for linegraphs and scatterplots). Figure \ref{fig:outlier} illustrates this result. 
 
\section{3. Estimation of Slope is Robust to Outliers}

\outlierFig
OLS regression operates under the assumption that there is a unimodal, symmetric distribution of residuals surrounding the line of best fit. Extreme outliers violate this assumption, and can result in trend lines that are substantially different from those produced by more robust methods. Visual inspection can accurately identify certain classes of outliers \cite{albers2014task}. Therefore, regression by eye may afford the estimation of both outlier-robust and outlier-sensitive trends. However, cognitive biases such as anchoring (the tendency to overweight the first set of information), availability (the tendency to overweight more recent or extreme information), and the hot-hand fallacy (the tendency to assume that trends of high or low values with continue) can impact how viewers categorize and utilize outliers in their estimations. These biases can impact forecasting tasks~\cite{campbell2009anchoring,ji2001culture}. That is, regression by eye may not uniformly weight outliers, depending on their position in the plot.

This experiment was largely identical to experiment 1 above, except we designated 0, 5, 10, or 15 points at the very beginning, first third, or end of the series as outliers, and placed them randomly in the top or bottom 10\% of the visual area of the visualization (whichever was farthest from the trend line). We then calculated the outlier-sensitive line of best fit, as well as the intersection between this new trend line and the original, outlier-less trend line. As with experiment 1, the participants controlled the slope of a rendered trend line with a slider. However, rather than being placed such that $f(0.5)=0.5$, we offset the trend line to the intersection of the robust (without outliers) and non-robust (including outliers) trend line. This allowed participants to express both types of fit with the same slider interaction, while affording estimations beyond a simple interpolation of both trends. This expressiveness is only afforded by linear fits of, and so we did not consider quadratic or trigonometric trends in this experiment.

Similar to the previous experiments, participants saw one of each of the three graph types, slope, and Gaussian residual bandwidth. The four outlier quantities $\{0,5,10,15\}$ was an additional factor. To maintain a manageable number of stimuli (piloting showed evidence of fatigue for more than 100 stimuli per trial), both the sign of the trend line (positive or negative), and the location of the outliers (beginning, first third, or end) were random factors, with each level apportioned to half and one third of the stimuli respectively. This resulted in 96 total stimuli, with an additional 4 validation stimuli otherwise excluded from analysis, in line with prior experiments.

\subsection{Hypotheses}

We had three hypotheses for the third experiment:
\begin{enumerate}
	\item \textbf{Participant estimation would be closer to an outlier-less robust trend line, than the non-robust OLS trend line.} That is, we assumed that in general, participants would ignore or downweight outliers when performing regression by eye.
	\item \textbf{As the number of outliers increased, estimations would be closer to the non-robust OLS fit.} We speculated on the existence of a ``tipping point'' of outlier density, beyond which participants would avoid robust fits, and interpolate between the robust and OLS fits.
	\item \textbf{Outliers at the end of the chart would result in estimations closer to the OLS fit than outliers in other locations on the plot.} Prior work on cognitive biases in forecasting tasks suggests that viewers may give more credence to more recent outliers (as indicative as new anchors points, or of an emerging ``streak''). We therefore believed that these points would be more heavily weighted, and the resulting estimation closer to the outlier-sensitive OLS line, than outliers ``earlier'' in the plot.
\end{enumerate}
\subsection{Results}

Our results support our first hypothesis. \textbf{Participant estimates were closer to the robust trend line than the trend line that included outliers}. Excluding conditions with zero outliers (and so the two trend lines would be identical), the trimmed mean of unsigned error was over $4$ times higher when calculated as a comparison to the OLS trend line ($0.36$) as opposed to the robust trend line ($0.08$). A Student's T-test confirmed that the error as defined by the OLS line was significantly higher than the robust line ($p<0.001$). However, a Student's T test found a significant difference in unsigned error when compared to the robust trend line between estimates where no outliers were present, and those with any amount of outliers ($M=0.07$ vs. $M=0.08$, $p<0.001$). This indicates that participants are not entirely ignoring outliers, although the small effect size suggestes that participants are giving outliers significantly less weight than OLS regression. 

%There is a confound between the number of outliers and the OLS trend line: since all outliers had approximately the same magnitude, more outliers create more extreme slopes with respect to the robust trend line, and so (if a participant were strictly ignoring outliers), larger differences in unsigned error. Similarly, outliers at the ends of the data series will adjust the slope of the OLS line more than outliers in the middle. We therefore measured performance as the unsigned error as a percentage of the difference between the slopes of the two types of fit. We performed a two-way ANOVA on the effect of outlier number and location on this percentage error measure. Participant id was included as a random variable.

%Our results fail to support our second hypothesis. \emph{Increasing number of outliers does not increase fidelity to the OLS trend line}. A one-way ANCOVA on the effect of outlier numbers greater than 0 on this percentage error showed that outliers were a significant main effect ($F(1,3438)=59$, $p<0.001$). However, the percentage error \emph{increases} as the number of outliers increase. A post-hoc Tukey's HSD shows that the percentage errors for 10 and 15 outliers are similar (trimmed means are $111\%$ and $112\%$ respectively), they are significantly different from the stimuli with only 5 outliers (trimmed mean of $97\%$). 


\section{Discussion}

In many cases, designers do not explicitly encoded regression information. In other cases, viewers may not have the statistical or graphical expertise to interpret such information, even when it is supplied. Yet, our results point to regression by eye as a robust and accurate method for estimating trends in bivariate data. Participants from a variety of backgrounds and levels of self-reported graphical and statistical expertise were capable of estimating both the slope and intercept of both linear and non-linear trends. That is, viewers of visualization are largely trustworthy when estimating the relationship between two variables in a plot.

However, this general accuracy at estimation trends is not universal. Area charts are visually asymmetrical: area below the line is filled in, and the area above it as not. This asymmetry creates within the bar bias: a systematic underestimation of the intercept of trends, due to the perceived higher likelihood of points in the filled-in area. Designers hoping to rely on regression by eye should avoid such asymmetries in their bivariate visualizations.

Likewise, viewers do not give the same weight to outliers as certain standard forms of statistical regression such as OLS. For noisy data, this robustness may be beneficial: humans themselves can be relied on to perform filtering operations without explicit guidance. However, in other cases, this insensitivity may result in visual estimations at odds with the statistical modeling underpinning other areas of analysis of a particular dataset. The emergence of new modes and the shifting of means may not result in a subsequent shift in beliefs about trends in data.

\subsection{Limitations \& Future Work}

Our experimental setup was intentionally simplistic, affording only a single free parameter for each experiment. In actual regression by eye, the viewer must simultaneously engage in multiple types of estimation: choosing a particular type of fit, filtering data, and then estimating the parameters of the chosen model. Errors in any one of these steps could compound, resulting in performance worse than our measures, where many of these decisions are presented as a given. 

There are also a number of design decisions not considered in this study that could impact regression by eye. For example, Wood et al.~\cite{wood2012sketchy} have shown that ``sketchiness'' in visualizations can result in skepticism of the data and design. It is possible that outliers could receive less visual (and so statistical) weight as a response to this sort of skepticism. Our stimuli likewise contained a sufficient number of points that bar charts were not a feasible choice of visualization. Given the propensity of bar charts to encourage comparison of individual, rather than aggregate quantities~\cite{zacks1999bars}, it is possible that bar charts of smaller scale bivariate data could promote fitting of local rather than global trends.

Finally, we focused on a simple form of regression, OLS, as our gold standard for measuring accuracy. While our data were constructed to satisfy the assumptions of OLS (with the exception of our outlier experiment), in most real world scenarios OLS is just one tool of many, and analysts must exercise judgment when determining how to fit their data. More complex forms of regression, and more complex fits, may not have ready analogues visually, and data concerning regression by eye may not extend to cover these cases.

Our future work is focused in three areas. Firstly, we wish to examine the impact of annotations relevant to regression (such as confidence bands, error bars, and curve boxplots~\cite{mirzargar2014curve}) on regression by eye. Can sufficient information promote caution in judgments of trend, or do initial visual estimations have too much weight to be overcome? Secondly, we wish to examine the impact of different rhetorical framings and presentations on regression by eye. Language from semiotics and rhetoric can provide testable structures for how visualizations are consumed~\cite{hullman2011visualization}: these framings (and so associated predispositions or biases about the data) could similarly impact statistical judgments. Lastly, we wish to examine techniques for overcoming bias in regression by eye and similar visual estimations of statistical quantities. Cognitive and perceptual biases are difficult to overcome, and may require exploration of novel forms of visualization \cite{micallef2012assessing}.

\subsection{Conclusion}

In this paper, we examine the ability of viewers of visualizations to perform regression by eye --- the visual estimation of trend in bivariate visualizations. We show that even viewers without statistical training can reliably estimate even non-linear trends in visualizations like scatterplots and linegraphs. However, area charts are subject to within the bar bias, and promote trends with lower intercepts than other bivariate visualizations. Regression by eye is also less sensitive to outliers than simple statistical regression techniques, resulting in a divergence between trends estimated by viewers, which do not give much weight to extreme outliers, and those trends calculated by simple statistical methods, can give equal weight all points in a dataset.


\section{Acknowledgments}

Omitted for review.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance{}


% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
