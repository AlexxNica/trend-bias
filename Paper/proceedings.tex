\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead
\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
% Some optional stuff you might like/need.
\usepackage{microtype} % Improved Tracking and Kerning
% \usepackage[all]{hypcap}  % Fixes bug in hyperref caption linking
\usepackage{ccicons}  % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of your draft document, you
% have to enable the "chi_draft" option for the document class. To do this, change the very first
% line to: "\documentclass[chi_draft]{sigchi}". You can then place todo notes by using the "\todo{...}"
% command. Make sure to disable the draft option again before submitting your final document.
\usepackage{todonotes}

\newcommand{\jeff}[1]{\textcolor{red}{JMH: #1}}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Regression by Eye: Estimating Trends in Bivariate Visualizations}
\def\plainauthor{Removed for Review}
\def\emptyauthor{}
\def\plainkeywords{Information Visualization, Graphical Perception, Regression}
\def\plaingeneralterms{Visualization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{2}
\author{%
%  \alignauthor{Michael Correll\\
%    \affaddr{University of Washington}\\
%    \email{mcorrell@cs.washington.edu}}\\
%  \alignauthor{Jeffrey Heer\\
%    \affaddr{University of Washington}\\
%    \email{jheer@cs.washington.edu}}\\
}

\maketitle

\begin{abstract}
Observing trends and predicting future values are common tasks for viewers of bivariate data visualizations. As many charts do not explicitly include trend lines or related statistical summaries, viewers often visually estimate trends directly from a plot. How reliable are the inferences viewers draw when performing such \emph{regression by eye}? Do particular visualization designs or data features bias trend perception? We present a series of crowdsourced experiments that assess the accuracy of trends estimated using regression by eye across a variety of bivariate visualizations, and examine potential sources of bias in these estimations. We find that viewers accurately estimate trends in many standard visualizations of bivariate data, but that both visual features (e.g., ``within the bar'' bias) and data features (e.g., the presence of outliers) can result in visual estimates that systematically diverge from standard least-squares regression models. In most cases regression by eye performs similarly to ordinary least squares regression, but there are edge cases where perceptual and statistical regression systematically diverge. These results suggest that, while regression by eye can be a reliable way of estimating trends, in some common cases these estimates systematically differ from statistical methods of regression.
\jeff{These last three sentences seem a bit overlapping / redundant. Plus we really seem to like the word ``systematically''.}
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous}

\keywords{\plainkeywords}

\input{./figures/figures.tex}

\section{Introduction}

%motivation here is poor. needs another turn of the crank.

An oft-cited example of the power of data visualization is Anscombe's quartet~\cite{anscombe1973graphs}, a set of four bivariate datasets with nearly identical summary statistics, but with qualitatively different patterns when drawn as four scatter plots. \jeff{Add figure of Anscombe's quartet...} This example relies on the fact that people have the ability to \emph{perceptually} estimate statistical quantities of interest. Visualization users regularly perform statistical tasks, including model selection, identification of outliers, and estimation of summary statistics, entirely through visual inspection. \jeff{Citations to perceptual work touching on the previous points? E.g., scatter plot means, color weaving, others?}

The estimation of trends in bivariate data is an important analytical task, as it is the basis for many factors relevant to decision-making, such as prediction, imputation, and comparison. However, model information is not always explicitly included by visualization designers. When designers do include trend information (for instance, by annotating a scatter plot with a line of best fit), other statistics relevant to the model (such as $r$ values or confidence bands) may be absent. Viewers must therefore perform visual estimation to gain a sense of any relevant statistics not provided.

Even if designers include all relevant information, the audience may lack the statistical expertise to interpret these values, or may be misled if the data violate modeling assumptions. As a further complication, the form of visual encoding may influence viewers' inferences. For instance, viewers may be more likely to consider trends with line charts, and to compare individual values with bar charts \cite{zacks1999bars}. Visual design choices can also introduce bias, such as the visual asymmetry of bars causing ``within-the-bar'' bias \cite{newman2012bar}. Designers would benefit from guidance regarding how accurately viewers make trend estimations by eye, and to what degree different visualization types might bias these estimations.

Knowing the strengths and limitations of such ``estimation by eye'' is therefore important for designers of data visualizations seeking to communicate statistical quantities, especially to a general audience. On the one hand, visual estimations may be \emph{inaccurate} for the use cases intended by a designer, or they may be \emph{biased}, leading to systematic over- or under-estimations. On the other hand, while visual estimation lacks the precision of formal statistics, it may be relatively unencumbered by modeling assumptions.

In this work, we describe a series of crowdsourced experiments on the visual estimation of trends in common bivariate visualizations such as scatter plots, area charts, and line graphs. We present the results of three studies investigating estimation of trend slope, trend intercept, and the effect of outliers. We find that, while in most cases viewers accurately estimate trends, area charts introduce systematic underestimation of trend intercept, and outliers introduce ambiguity in how trends are estimated. \jeff{This is itself a bit ambiguous. What do you mean by ``introduce ambiguity''? Can you be more precise?} These results suggest that there are several areas where human judgments diverge from the fitted models generated by techniques such as Ordinary Least Squares (OLS), and that the design of bivariate visualizations can introduce additional biases in these judgments. \jeff{Obligatory design implications remark here?}

\section{Related Work}

While there is a great deal of foundational work in visualization and graphical perception dealing with the estimation of individual values in visualizations (such as the height of the bar in a bar chart, or the angle of a line in a line graph), there is comparatively little work on how viewers of visualizations perceive aggregate statistical quantities.

Ariely~\cite{ariely2001seeing} suggests that, in concert with the perception of individual objects, we also collect information about the \emph{ensemble} properties of visual displays. Szafir et al.~\cite{szafir2016four} note that this \emph{ensemble coding} might afford relatively accurate estimation of \emph{summary statistics} in visualizations. However, visualizations with good performance for summary tasks may not result in good performance at point tasks, and vice versa~\cite{albers2014task,fuchs2013evaluation}. A further difficulty is that tasks requiring estimation of values in visualizations (where there is a single correct answer) are qualitatively different from tasks requiring predictions (where differing mental models and priors can result in a multitude of potential valid responses).

Scatter plots are a standard means of visualizing bivariate data, with a multitude of design parameters that affect their suitability for aggregate tasks~\cite{cleveland1984many}. Prior work has confirmed that viewers can make use of scatter plots to perform prediction tasks (which tacitly rely on estimation of trend) in ways that are robust to both noise~\cite{harvey1997effects} and problem frame~\cite{lewandowsky2011popular}. However, the heuristics used when performing these prediction tasks (such as the anchor-and-adjust method~\cite{bolger1993context}) can introduce biases.

Similarly, visual design choices (for instance, the decision to encode class membership with color or with shape) can also impact performance at aggregate tasks~\cite{gleicher2013perception,lewandowsky1989discriminating}. The aspect ratio of graphs can also bias judgments about trends~\cite{beattie2002impact}\,---\,wider aspect ratios can cause viewers to underestimate the severity of effects, when compared to narrower charts. Another bias in prediction tasks is the ``within-the-bar'' bias \cite{newman2012bar}: for visually asymmetric visualizations such as bar charts, points contained within the visual area of the bar glyph are perceived as likelier than those outside the glyph.

Recent work in the visualization community has focused on the perception of correlation in scatter plots. Rensink et al.~\cite{rensink2010perception} show that viewers can estimate correlation with some accuracy in scatter plots. Harrison et al.~\cite{harrison2014ranking} extend this finding to other visualization types, although a re-analysis by Kay \& Heer~\cite{kay2016beyond} indicates that, for many of the more esoteric bivariate visualizations, performance at this task is poor. Estimation of correlation can also be biased by the choice of scales~\cite{cleveland1982variables}\,---\,the whitespace and aspect ratio changes introduced by expanding the scale of the axes can cause overestimation of correlation.

Our task of trend estimation combines elements of both prediction and correlation estimation tasks. As with prediction, there is not necessarily an unambiguous correct estimation of trend (different modeling and regression methods can produce different trend lines). As with correlation, viewers must make holistic judgments about the dataset in a way that (as per Harrison et al.~\cite{harrison2014ranking}) likely relies on a set of key visual proxies. \jeff{What do you mean by ``key visual proxies''? Be clearer so that readers can follow along.} And, as with both tasks, we believe that trend estimation can be biased through conscious or unconscious design choices.
\jeff{Such as? The related work could end on a stronger note if we put forward some concrete hypotheses and note that prior work has not addressed them...}

\section{General Methods}

\expFig

In order to assess the ability of visualization viewers to estimate trends, we conducted a series of three crowdsourced experiments on Amazon's Mechanical Turk platform. We designed these experiments to establish a performance baseline for regression by eye, and to examine potential sources of bias for these estimates. Crowdsourcing is a valuable tool for graphical perception experiments, as it affords flexibilities and scales that would be difficult for in-person experiments~\cite{heer2010crowdsourcing}. Crowdsourced graphical perception experiments produce results that are largely in keeping with prior, lab-based work~\cite{heer2010crowdsourcing, talbot2014four}. In this section, we describe experimental design aspects shared across all experiments.

\subsection{Participants}

We limited the Mechanical Turk participant pool to subjects from within the United States, with a prior task approval rating of at least 90\%. For validation, certain trials contained the actual (OLS) trend line. We excluded the results of participants with poor performance on these validation questions. Across the three reported experiments and pilots, we performed 7 such exclusions. We recruited additional participants to replace these excluded subjects. Based on timings from internal piloting, we paid each participant \$2 for their participation, for a target rate of \$8/hour.

We analyzed data from 48 participants for each experiment (excluding rejections), for a total of 144 participants (98 male, 43 female, 3 who declined to state; $\mu_{age}= 33.2$, $\sigma_{age}=8.8$). Across the subject pool, 10 reported having graduate or professional degrees, 69 college degrees, 40 at least some college, and 25 high school diplomas. After completing the main experimental task, participants were asked to self-assess their familiarity with charts and graphs on a 5 point Likert scale. The plurality (64) rated themselves as ``3. Some familiarity,'' and none rated themselves with the maximum rating of ``5. A great deal of experience.''

\subsection{Experimental Interface}

In each experiment, we presented participants with visualizations of bivariate data and asked them to interactively adjust trend lines to best fit that data. Participants responded using a slider without tick marks, in order to limit anchoring effects~\cite{matejka2016effect}. Moving the slider adjusted a purple trend line by modifying its slope (in experiments 1 and 3) or its y-intercept (in experiment 2). Fig. \ref{fig:experiment} shows an example experimental task. After moving the slider, participants had to confirm their choice of trend line. The primary dependent measure is accuracy (e.g., the difference between the subject-specified slope and the slope of the series if the residuals were removed). We chose this design over a standard binary forced choice design for its greater expressiveness, as well as for the interactive feedback of adjusting the fit by hand. We avoided more expressive designs (such as freeform drawing or other elicitation techniques) in order to constrain the model space and to reduce the impact of factors such as accuracy of motor movements or heterogeneity of input devices.

\subsection{Data Generation}

For each stimulus we wished to have precise, independent control over relevant statistics such as noise, slope, and sample size, while maintaining the appearance of a ``natural'' distribution of points. Existing methods for generating points for related experimental tasks such as estimation of correlations (e.g., Harrison et al.~\cite{harrison2014ranking}) do not afford independence: e.g., Pearson's $r$ is correlated with the slope, as points with identical residuals but different slopes of their linear fits will have differing $r$ values. We also wished to have a fair comparison between visual estimation and the results of ordinary least squares (OLS) regression. We therefore used the standard model of OLS to generate points, namely that $y=\beta x + \epsilon$, where $\epsilon$ is a normally-distributed error term.

For each point set, we created a set of residual values, sampled evenly from a discretized Gaussian. This set was permuted, and then applied to the points along with a target trend. As heteroskedasticity introduced through permutation could alter the actual trend away from the target trend, we performed rejection sampling to ensure that the trend of the resulting points were within $10^{-7}$ of the target trend. We reused these residuals across all different trend types (linear, quadratic, or trigonometric). Except where noted, we selected trend lines that were centered in the image: that is, for a horizontal data extent of $[0,1]$, $f(0.5) = 0.5$. In all experiments, we desired control over the direction of the trend. For linear fits, this is the slope of the trend line. For quadratic fits, this was the curvature, as controlled by the coefficient of the second degree term. For trigonometric fits, this was the amplitude of a cosine function (with negative amplitude corresponding to negative slopes, and positive to positive slopes). These terms create similar relationships, such that a value of $0$ is the line $f(x)=0.5$, a value of $1$ goes from $f(0)=0$ to $f(1)=1$, and $-1$ has the inverse relationship.


\section{Experiment 1: Slope Estimation}

We designed our first experiment to examine how accurate participants were at estimating the magnitude (slope, amplitude, or curvature) of trends in bivariate visualizations. We examined a variety of common bivariate visualizations \jeff{Such as?}. In addition to linear trends, we examined more complex relationships such as quadratic and trigonometric functions.

We presented participants with a series of bivariate visualizations, who adjusted a slider to fit the perceived trend in the points. The visualizations were one of a scatter plot, line graph, or area chart (with the filled area below the line). The slider parameterized one of three types of trend (linear, quadratic, or trigonometric). For each stimulus, participants adjusted a slider that controlled the slope of a rendered trend line (or, in the case of the quadratic trends, the curvature, or the trigonometric, the positive/negative amplitude). \jeff{To many parentheticals...}

Participants saw one of each combination of 3 chart types (scatter plot, line chart, or area chart), 8 possible slopes $\beta = \pm \{0.1,0.2,0.4,0.8\}$, and 4 bandwidths of Gaussian residuals $\sigma = \{0.05,0.1,0.15,0.2\}$, for a total of 96 stimuli. We also included an additional 4 validation stimuli otherwise excluded from analysis. The type of trend (linear, quadratic, or trigonometric) was a random factor, with 32 stimuli of each factor level. \jeff{Was this factor random or consistent across subjects?}

\subsection{Hypotheses}

We had three hypotheses for the first experiment:
\begin{enumerate}
	\item \textbf{As the bandwidth of the residuals increased, accuracy would decrease}. Increasing the bandwidth of the residuals results in a lower correlation coefficient and higher perceived noise in the bivariate data. Prior work indicates that these related measures correspond to decreased accuracy for aggregate tasks in bivariate visualizations~\cite{albers2014task, harrison2014ranking}.
	\item \textbf{More complex relationships would result in lower accuracy}. Quadratic and trigonometric relationships are visually more complex than linear relationships, and often require more complex statistical methods to analyze. We anticipated that estimation of these less familiar relationships would therefore be more difficult than the linear case.
	\item \textbf{Estimations would be unbiased}. That is, there would be no systematic over- or underestimation of trends.
\end{enumerate}

\subsection{Results}
We performed a three-way analysis of covariance (ANCOVA) of the effect of residual bandwidth, graph type and trend type on unsigned error \jeff{absolute error?} in estimation of trend lines. We included participant ids and the actual slope of the trend line as covariates.

Our results support our first hypothesis: \textbf{larger residuals reduce accuracy at regression by eye}.  We observed a significant main effect for the bandwidth of the Gaussian used to generate residuals ($F(1,4554)=950$, \jeff{p-value?}). The trimmed mean \jeff{trimmed how?} of absolute error increased monotonically with this bandwidth, from $7\%$ of the actual slope of the trend line when the bandwidth was $0.05$, to $32\%$ when the bandwidth was $0.20$. Figure \ref{fig:sigma} illustrates these results.

Our results fail to support our second hypothesis: \textbf{there was no statistically significant difference in estimation accuracy among linear, quadratic, or trigonometric trends}. Fit type had only a marginal main effect ($F(2,4554)=2.60$, $p=0.074$), and a post-hoc tests (using Tukey's Honest Significant Difference) did not identify any significant pairwise-interactions. The trimmed mean of absolute estimation error of linear fits was $17\%$ of the actual slope, in the middle of the $16\%$ error for quadratic fits, and $18\%$ for trignometric fits, providing evidence that the relative unfamiliarity of non-linear trends does not have a notable impact on performance.

Our results support our third hypothesis: \textbf{there was no statistically significant bias in estimations}. Participants saw an equal number of stimuli with positive and negative trend lines. If estimates of these trends were unbiased, we would expect the average signed error to be close to zero. The average signed error was $0.0008$, far less than the fidelity of the slider used to input guesses ($\Delta=0.01$). A Student's T-test failed to support the hypothesis that $mu_{error}\ne0$ ($p=0.70$) \jeff{Test statistic and degrees of freedom?}.

\section{Experiment 2: ``Within-the-Bar'' Bias}

``Within the bar'' bias is a known perceptual bias involving bar charts, in which points contained in the visual area of the glyph of the bar are deemed likelier than points outside of the glyph. Newman \& Scholl~\cite{newman2012bar} encountered this bias for a sampling task: ``how likely is this point to have been drawn from the distribution represented by this bar?'' Correll \& Gleicher~\cite{correll2014error} likewise encountered this bias for inferential tasks: ``how likely is the population mean to take a particular value, given the sample represented by this bar?''

We hypothesized that this bias would likewise occur for regression tasks when using visually asymmetric visualizations such as area charts. The slope estimation task in the previous experiment would not capture this bias, as there is no method for participants to indicate a \emph{uniform} underestimation in trends; decreasing the slope would cause underestimation at the beginning of the plot but not the end, and vice versa. We therefore designed this experiment to elicit estimates of the y-intercepts of trends. A within-the-bar bias would then appear as systematic underestimation of intercept in area charts.

As with the previous experiment, we presented participants with a series of bivariate visualizations. However, instead of estimating the \emph{slope} of the points, participants estimated the \emph{y-intercept} of the trend line. For each trial, we added a uniform offset to the points in the bivariate visualization in the data range $[-0.25,0.25]$. The rendered trend line was initially placed with the correct slope, and such that $f(0.5)=0.5$ in data space. Participants adjusted a slider controlling the vertical offset of this trend line.

The plots had the same factor levels as the previous experiment, for a total of 96 stimuli per participant, with an additional 4 validation stimuli otherwise excluded from analysis. The uniform offset was an additional random factor for each stimulus.

\subsection{Hypotheses}

We had one hypothesis for the second experiment:
\begin{enumerate}
	\item \textbf{Area charts would be subject to within the bar bias.} That is, participants would estimate lower values of y-intercepts of trends than with line charts and scatter plots.
\end{enumerate}

\subsection{Results}
We performed a one-way ANCOVA of the effect of graph type on \emph{signed} error. We included residual bandwidth as a covariate, and participant ids as a random factor.

Our results support our first hypothesis: \textbf{participants systematically overestimated the intercept of trends in area charts, but not in scatter plots or line graphs}. We observed a significant main effect of graph type on signed error ($F(2,2431)=27$, $p<0.001$). A post-hoc Tukey's HSD confirmed significant differences in error between the area chart and the other two chart types, but not between scatter plots and line graphs. The trimmed mean \jeff{trimmed how?} of the signed error of estimations made with area charts was an underestimation of $-0.02$, compared to a trimmed mean of $0.002$ for the other two conditions. This underestimation corresponded to unsigned errors more than twice as large in area charts (trimmed mean of $0.04$ for intercepts that ranged from $\{-0.25,0.25\}$, compared to $0.02$ for line graphs and scatter plots). Figure \ref{fig:outlier} illustrates this result.

\section{Experiment 3. Estimation involving Outliers}

\outlierFig
OLS regression operates under the assumption that there is a unimodal, symmetric distribution of residuals surrounding the line of best fit. Extreme outliers violate this assumption, and can result in trend lines that are substantially different from those produced by more robust methods. Visual inspection can accurately identify certain classes of outliers \cite{albers2014task}. Therefore, regression by eye may afford the estimation of both outlier-robust and outlier-sensitive trends. However, cognitive biases such as anchoring (the tendency to overweight the first set of information), availability (the tendency to overweight more recent or extreme information), and the hot-hand fallacy (the tendency to assume that trends of high or low values with continue) can impact how viewers categorize and utilize outliers in their estimations \jeff{Need citations in this sentence?}. These biases can impact forecasting tasks~\cite{campbell2009anchoring, ji2001culture}. That is, regression by eye may not uniformly weight outliers, depending on their position in the plot.

This experiment was largely identical to experiment 1 above, except we designated 0, 5, 10, or 15 points at the very beginning, first third, or end of the series as outliers, and placed them randomly in the top or bottom 10\% of the visual area of the visualization (whichever was farthest from the trend line). We then calculated the outlier-sensitive line of best fit, as well as the intersection between this new trend line and the original, outlier-less trend line. As with experiment 1, the participants controlled the slope of a rendered trend line with a slider. However, rather than being placed such that $f(0.5)=0.5$, we offset the trend line to the intersection of the robust (without outliers) and non-robust (including outliers) trend line. This allowed participants to express both types of fit with the same slider interaction, while affording estimations beyond a simple interpolation of both trends. This expressiveness is only afforded by linear fits of, and so we did not consider quadratic or trigonometric trends in this experiment.

Similar to the previous experiments, participants saw one of each of the three graph types, slope, and Gaussian residual bandwidth. The four outlier quantities $\{0,5,10,15\}$ was an additional factor. To maintain a manageable number of stimuli (piloting showed evidence of fatigue for more than 100 stimuli per trial), both the sign of the trend line (positive or negative), and the location of the outliers (beginning, first third, or end) were random factors, with each level apportioned to half and one third of the stimuli respectively. This resulted in 96 total stimuli, with an additional 4 validation stimuli otherwise excluded from analysis, in line with prior experiments.

\subsection{Hypotheses}

We had three hypotheses for the third experiment:
\begin{enumerate}
	\item \textbf{Participant estimation would be closer to a robust trend line ignoring outliers, than the non-robust OLS trend line.} That is, we assumed that in general, participants would ignore or downweight outliers when performing regression by eye.
	\item \textbf{As the number of outliers increased, estimations would be closer to the non-robust OLS fit.} We speculated on the existence of a ``tipping point'' of outlier density, beyond which participants would avoid robust fits, and interpolate between the robust and OLS fits.
	\item \textbf{Outliers at the end of the chart would result in estimations closer to the OLS fit than outliers in other locations on the plot.} Prior work on cognitive biases in forecasting tasks suggests that viewers may give more credence to more recent outliers (as indicative as new anchors points, or of an emerging ``streak'') \jeff{cite?}. We therefore believed that these points would be more heavily weighted, and the resulting estimation closer to the outlier-sensitive OLS line, than outliers ``earlier'' in the plot.
\end{enumerate}

\subsection{Results}

Our results support our first hypothesis. \textbf{Participant estimates were closer to the robust trend line than the trend line that included outliers}. Excluding conditions with zero outliers (and so the two trend lines would be identical), the trimmed mean \jeff{trimmed how?} of unsigned error was over $4$ times higher when calculated as a comparison to the OLS trend line ($0.36$) as opposed to the robust trend line ($0.08$). A Student's T-test confirmed that the error as defined by the OLS line was significantly higher than the robust line ($p<0.001$) \jeff{test statistic, dof}. However, a Student's T-test found a significant difference in unsigned error when compared to the robust trend line between estimates where no outliers were present, and those with any amount of outliers ($\mu=0.07$ vs. $\mu=0.08$, $p<0.001$). This indicates that participants are not entirely ignoring outliers, although the small effect size suggestes that participants are giving outliers significantly less weight than OLS regression.

\jeff{Whither the other hypotheses?}

%There is a confound between the number of outliers and the OLS trend line: since all outliers had approximately the same magnitude, more outliers create more extreme slopes with respect to the robust trend line, and so (if a participant were strictly ignoring outliers), larger differences in unsigned error. Similarly, outliers at the ends of the data series will adjust the slope of the OLS line more than outliers in the middle. We therefore measured performance as the unsigned error as a percentage of the difference between the slopes of the two types of fit. We performed a two-way ANOVA on the effect of outlier number and location on this percentage error measure. Participant id was included as a random variable.

%Our results fail to support our second hypothesis. \emph{Increasing number of outliers does not increase fidelity to the OLS trend line}. A one-way ANCOVA on the effect of outlier numbers greater than 0 on this percentage error showed that outliers were a significant main effect ($F(1,3438)=59$, $p<0.001$). However, the percentage error \emph{increases} as the number of outliers increase. A post-hoc Tukey's HSD shows that the percentage errors for 10 and 15 outliers are similar (trimmed means are $111\%$ and $112\%$ respectively), they are significantly different from the stimuli with only 5 outliers (trimmed mean of $97\%$).


\section{Discussion}

In many cases, designers do not explicitly encode regression information. In other cases, viewers may not have the statistical or graphical expertise to interpret such information, even when it is supplied. Yet, our results point to regression by eye as a robust and accurate method for estimating trends in bivariate data. Participants from a variety of backgrounds and levels of self-reported graphical and statistical expertise were capable of estimating both the slope and intercept of both linear and non-linear trends. That is, viewers of visualization are largely trustworthy when estimating the relationship between two variables in a plot.
\jeff{This point was not so clear in the results discussion of experiment 1. We should discuss (and visualize) the range of accuracies observed there.}

However, this general accuracy of trend estimation is not universal. Area charts are visually asymmetrical: the area below the line is filled in, and the area above it is not. We found that this asymmetry creates a within the bar bias: a systematic underestimation of the intercept of trends, due to the perceived higher likelihood of points in the filled-in area. Designers hoping to rely on regression by eye should avoid such asymmetries in their bivariate visualizations.

Likewise, viewers do not give the same weight to outliers as OLS regression, but they do not ignore them either. For noisy data, this robustness may be beneficial: people can be relied on to perform filtering operations without explicit guidance. However, in other cases, this insensitivity may result in visual estimations at odds with the statistical modeling underpinning other areas of analysis of a particular dataset \jeff{What are you trying to say here?}. The emergence of new modes and the shifting of means may not result in a subsequent shift in beliefs about trends in data. \jeff{???}

\subsection{Limitations \& Future Work}

Our experimental setup was intentionally simple, affording only a single free parameter for each experiment. In actual regression by eye, the viewer may simultaneously engage in multiple types of estimation: choosing a particular type of fit, ignoring outliers, and estimating the parameters of the chosen model. Errors in any one of these steps could compound, resulting in performance worse than our measures, where many of these decisions are fixed.

There are also a number of design decisions not considered in this study that could impact regression by eye. For example, Wood et al.~\cite{wood2012sketchy} have shown that ``sketchiness'' in visualizations can result in skepticism of the data and design. It is possible that outliers could receive less visual (and so statistical) weight as a response to this sort of skepticism. Our stimuli likewise contained a sufficient number of points that bar charts were not a feasible choice of visualization. Given the propensity of bar charts to encourage comparison of individual, rather than aggregate quantities~\cite{zacks1999bars}, it is possible that bar charts of smaller scale bivariate data could promote fitting of local rather than global trends.

Finally, we focused on a simple form of regression, OLS, as our standard for measuring accuracy. While our data were constructed to satisfy the assumptions of OLS (with the intentional exception of our outlier experiment), in most real world scenarios OLS is just one tool of many, and analysts must exercise judgment when determining how to fit their data. More complex models may not have ready visual analogues, and data concerning regression by eye may not extend to cover these cases.

Our future work is focused in three areas. First, we wish to examine the impact of annotations relevant to regression (such as confidence bands, error bars, and curve boxplots~\cite{mirzargar2014curve}) on regression by eye. Can sufficient information promote caution in judgments of trend, or do initial visual estimations have too much weight to be overcome? Second, we wish to examine the impact of different rhetorical framings and presentations on regression by eye. Language from semiotics and rhetoric can provide testable structures for how visualizations are consumed~\cite{hullman2011visualization}: these framings (and so associated predispositions or biases about the data) could similarly impact statistical judgments. \jeff{Can you clarify this last part? And/or provide a concrete example?} Lastly, we wish to examine techniques for overcoming bias in regression by eye and similar visual estimations of statistical quantities. Cognitive and perceptual biases are difficult to overcome, and may require exploration of novel forms of visualization~\cite{micallef2012assessing}.

\subsection{Conclusion}

In this paper, we examine the ability of viewers of visualizations to perform \emph{regression by eye}\,---\,the visual estimation of trends in bivariate visualizations. We show that even viewers without statistical training can reliably estimate both linear and non-linear trends in visualizations such as scatter plots and line graphs. However, area charts are subject to a ``within the bar'' bias, leading to estimated trends with lower intercepts than other bivariate visualizations. Regression by eye is also less sensitive to outliers than simple statistical regression techniques, resulting in a divergence between trends estimated by viewers, which do not give much weight to extreme outliers, and those trends calculated by simple statistical methods, can give equal weight all points in a dataset. \jeff{Run-on sentence. Also OLS does \emph{not} give equal weight to all points, such is the nature of \textbf{squared} error.}


\section{Acknowledgments}

Omitted for review.

% Balancing columns in a ref list is a bit of a pain because you
% either use a hack like flushend or balance, or manually insert
% a column break.  http://www.tex.ac.uk/cgi-bin/texfaq2html?label=balance
% multicols doesn't work because we're already in two-column mode,
% and flushend isn't awesome, so I choose balance.  See this
% for more info: http://cs.brown.edu/system/software/latex/doc/balance.pdf
%
% Note that in a perfect world balance wants to be in the first
% column of the last page.
%
% If balance doesn't work for you, you can remove that and
% hard-code a column break into the bbl file right before you
% submit:
%
% http://stackoverflow.com/questions/2149854/how-to-manually-equalize-columns-
% in-an-ieee-paper-if-using-bibtex
%
% Or, just remove \balance and give up on balancing the last page.
%
\balance{}


% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
